{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 0: import module for bert_QA project</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 0\n",
    "import json\n",
    "import os\n",
    "from dataset import read_jsonl\n",
    "import re\n",
    "# step 4\n",
    "from transformers import AutoTokenizer, BertForQuestionAnswering\n",
    "from transformers import AdamW\n",
    "# step 6\n",
    "import torch as th\n",
    "\n",
    "os.chdir('/home/sangmin/grade-school-math/grade_school_math/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 1: Retrieve and Store the data</h1>\n",
    "Here I take and store the texts, queries and answers from the train and validation .json files. I save these informations into lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(\"data/train.jsonl\")\n",
    "objs = read_jsonl(path)\n",
    "\n",
    "texts = []\n",
    "queries = []\n",
    "answers = []\n",
    "\n",
    "for i in objs:\n",
    "\n",
    "    answer_info = {}\n",
    "\n",
    "    an = i['answer']\n",
    "    co = re.findall('.+\\n', an)\n",
    "    ans = re.findall('\\d+$', an)\n",
    "\n",
    "    context = co[0]\n",
    "    answer_info['text'] = ans[0]\n",
    "    answer_info['answer_start'] = context.rfind('>>') + 2\n",
    "    question = i['question']\n",
    "        \n",
    "    texts.append(context)\n",
    "    answers.append(answer_info)\n",
    "    queries.append(question)\n",
    "\n",
    "train_texts, train_queries, train_answers = texts, queries, answers\n",
    "\n",
    "#{'question': 'Nina enjoys keeping insects as pets. She has 3 spiders and 50 ants. Each spider has 8 eyes. Each ant has 2 eyes. How many eyes are there in total among Nina’s pet insects?', \n",
    "#'answer': 'The number of eyes from the spiders is 3 * 8 = <<3*8=24>>24 eyes\\nThe number of eyes from the ants is 50 * 2 = <<50*2=100>>100 eyes\\nThe total number of eyes among Nina’s insects is 24 + 100 = <<24+100=124>>124 eyes\\n#### 124'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(\"data/test.jsonl\")\n",
    "objs = read_jsonl(path)\n",
    "\n",
    "texts = []\n",
    "queries = []\n",
    "answers = []\n",
    "\n",
    "for i in objs:\n",
    "\n",
    "    answer_info = {}\n",
    "\n",
    "    an = i['answer']\n",
    "    co = re.findall('.+\\n', an)\n",
    "    ans = re.findall('\\d+$', an)\n",
    "\n",
    "    context = co[0]\n",
    "    answer_info['text'] = ans[0]\n",
    "    answer_info['answer_start'] = context.rfind('>>') + 2\n",
    "    question = i['question']\n",
    "        \n",
    "    texts.append(context)\n",
    "    answers.append(answer_info)\n",
    "    queries.append(question)\n",
    "\n",
    "val_texts, val_queries, val_answers = texts, queries, answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Step 2: Check the data</H1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we have 7473 passages, queries and answers from the training data. The answer is stored in a dictionary with the specific answer in the \"text\" cell and the accurate character index that the answer is started in cell \"answer start\". As we observe, we need to fill the information about the exact index of the character that the answer is ending from the referance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7473\n",
      "7473\n",
      "7473\n"
     ]
    }
   ],
   "source": [
    "print(len(train_texts))\n",
    "print(len(train_queries))\n",
    "print(len(train_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passage:  ['Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\n', 'Weng earns 12/60 = $<<12/60=0.2>>0.2 per minute.\\n']\n",
      "Query:  ['Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?']\n",
      "Answer:  [{'text': '72', 'answer_start': 31}, {'text': '10', 'answer_start': 33}]\n"
     ]
    }
   ],
   "source": [
    "print(\"Passage: \",train_texts[0:2])  \n",
    "print(\"Query: \",train_queries[0:2])\n",
    "print(\"Answer: \",train_answers[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passage:  Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day.\n",
      "\n",
      "Query:  Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
      "Answer:  {'text': '18', 'answer_start': 37}\n"
     ]
    }
   ],
   "source": [
    "print(\"Passage: \",val_texts[0])  \n",
    "print(\"Query: \",val_queries[0])\n",
    "print(\"Answer: \",val_answers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 3: Find the end position character</h1>\n",
    "Because Bert model needs both start and end position characters of the answer, I have to find it and store it for later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find end position character in train data.\n",
    "for answer, text in zip(train_answers, train_texts):\n",
    "    real_answer = answer['text']\n",
    "    start_idx = answer['answer_start']\n",
    "    end_idx = start_idx + len(real_answer)\n",
    "    answer['answer_end'] = end_idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find end position character in valid data.\n",
    "for answer, text in zip(val_answers, val_texts):\n",
    "    real_answer = answer['text']\n",
    "    start_idx = answer['answer_start']\n",
    "    end_idx = start_idx + len(real_answer)\n",
    "    answer['answer_end'] = end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passage:  ['Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\n', 'Weng earns 12/60 = $<<12/60=0.2>>0.2 per minute.\\n']\n",
      "Query:  ['Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?']\n",
      "Answer:  [{'text': '72', 'answer_start': 31, 'answer_end': 33}, {'text': '10', 'answer_start': 33, 'answer_end': 35}]\n"
     ]
    }
   ],
   "source": [
    "#double check for data including answer_end index\n",
    "print(\"Passage: \",train_texts[0:2])  \n",
    "print(\"Query: \",train_queries[0:2])\n",
    "print(\"Answer: \",train_answers[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 4: Tokenize passages and queries</h1>\n",
    "In this task is asked to select the BERT-base pretrained model “bert-base-uncased” for the tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d4d136eda64b3f986ac9a5f245bf0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9269ac60a33b4fdcb44a02364c55c704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcbb230e80414fc6aedb472ded3128e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f53ca1d3fa54db7998fc81755a05e42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "train_encodings = tokenizer(train_texts, train_queries, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, val_queries, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 5: Convert the start-end positions to tokens start-end positions</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "def add_token_positions(encodings, answers):\n",
    "  start_positions = []\n",
    "  end_positions = []\n",
    "\n",
    "  count = 0\n",
    "\n",
    "  for i in range(len(answers)):\n",
    "    start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
    "    end_positions.append(encodings.char_to_token(i, answers[i]['answer_end']))\n",
    "\n",
    "    # if start position is None, the answer passage has been truncated\n",
    "    if start_positions[-1] is None:\n",
    "      start_positions[-1] = tokenizer.model_max_length\n",
    "      \n",
    "    # if end position is None, the 'char_to_token' function points to the space after the correct token, so add - 1\n",
    "    if end_positions[-1] is None:\n",
    "      end_positions[-1] = encodings.char_to_token(i, answers[i]['answer_end'] - 1)\n",
    "      # if end position is still None the answer passage has been truncated\n",
    "      if end_positions[-1] is None:\n",
    "        count += 1\n",
    "        end_positions[-1] = tokenizer.model_max_length\n",
    "\n",
    "  print(count)\n",
    "\n",
    "  # Update the data in dictionary\n",
    "  encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "\n",
    "add_token_positions(train_encodings, train_answers)\n",
    "add_token_positions(val_encodings, val_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 6: Create a Dataset class</h1>\n",
    "Create a Squatdataset class (inherits from torch.utils.data.Dataset), that helped me to train and validate my previous data more easily and convert encodings to datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadDataset(th.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_dataset = SquadDataset(train_encodings)\n",
    "val_dataset = SquadDataset(val_encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 7: Use of DataLoader</h1>\n",
    "I put my previous data to DataLoader, so as to split them in \"pieces\" of 8 batch size. I will explain the selection of this value of batch size later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bcdefg\n"
     ]
    }
   ],
   "source": [
    "s_list = ['abc', 'bcd', 'bcdefg', 'abba', 'cddc', 'opq']\n",
    " \n",
    "print(max(s_list, key=lambda s: len(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2abf55bda1c53852352d4e16de7f91e46e9520816b382a01d9abf8d98ce130e3"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
